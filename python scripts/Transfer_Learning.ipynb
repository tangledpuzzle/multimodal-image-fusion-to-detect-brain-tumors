{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.4.0+cpu in c:\\users\\rebec\\anaconda3\\lib\\site-packages (1.4.0+cpu)\n",
      "Requirement already satisfied: torchvision==0.5.0+cpu in c:\\users\\rebec\\anaconda3\\lib\\site-packages (0.5.0+cpu)\n",
      "Requirement already satisfied: six in c:\\users\\rebec\\anaconda3\\lib\\site-packages (from torchvision==0.5.0+cpu) (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rebec\\anaconda3\\lib\\site-packages (from torchvision==0.5.0+cpu) (1.16.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\rebec\\anaconda3\\lib\\site-packages (from torchvision==0.5.0+cpu) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vgg import vgg19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(VGG19, self).__init__()\n",
    "        features = list(vgg19(pretrained=True).features)\n",
    "        if device == \"cuda\":\n",
    "            self.features = nn.ModuleList(features).cuda().eval()\n",
    "        else:\n",
    "            self.features = nn.ModuleList(features).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = []\n",
    "        for idx, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if idx == 3:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion:\n",
    "    def __init__(self, input):\n",
    "        \"\"\"\n",
    "        Class Fusion constructor\n",
    "\n",
    "        Instance Variables:\n",
    "            self.images: input images\n",
    "            self.model: CNN model, default=vgg19\n",
    "            self.device: either 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        self.input_images = input\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = VGG19(self.device)\n",
    "\n",
    "    def fuse(self):\n",
    "        \"\"\"\n",
    "        A top level method which fuse self.images\n",
    "        \"\"\"\n",
    "        # Convert all images to YCbCr format\n",
    "        self.normalized_images = [-1 for img in self.input_images]\n",
    "        self.YCbCr_images = [-1 for img in self.input_images]\n",
    "        for idx, img in enumerate(self.input_images):\n",
    "            if not self._is_gray(img):\n",
    "                self.YCbCr_images[idx] = self._RGB_to_YCbCr(img)\n",
    "                self.normalized_images[idx] = self.YCbCr_images[idx][:, :, 0]\n",
    "            else:\n",
    "                self.normalized_images[idx] = img / 255.\n",
    "        # Transfer all images to PyTorch tensors\n",
    "        self._tranfer_to_tensor()\n",
    "        # Perform fuse strategy\n",
    "        fused_img = self._fuse()[:, :, 0]\n",
    "        # Reconstruct fused image given rgb input images\n",
    "        for idx, img in enumerate(self.input_images):\n",
    "            if not self._is_gray(img):\n",
    "                self.YCbCr_images[idx][:, :, 0] = fused_img\n",
    "                fused_img = self._YCbCr_to_RGB(self.YCbCr_images[idx])\n",
    "                fused_img = np.clip(fused_img, 0, 1)\n",
    "\n",
    "        return (fused_img * 255).astype(np.uint8)\n",
    "        # return fused_img\n",
    "\n",
    "    def _fuse(self):\n",
    "        \"\"\"\n",
    "        Perform fusion algorithm\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            imgs_sum_maps = [-1 for tensor_img in self.images_to_tensors]\n",
    "            for idx, tensor_img in enumerate(self.images_to_tensors):\n",
    "                imgs_sum_maps[idx] = []\n",
    "                feature_maps = self.model(tensor_img)\n",
    "                for feature_map in feature_maps:\n",
    "                    sum_map = torch.sum(feature_map, dim=1, keepdim=True)\n",
    "                    imgs_sum_maps[idx].append(sum_map)\n",
    "\n",
    "            max_fusion = None\n",
    "            for sum_maps in zip(*imgs_sum_maps):\n",
    "                features = torch.cat(sum_maps, dim=1)\n",
    "                weights = self._softmax(F.interpolate(features,\n",
    "                                        size=self.images_to_tensors[0].shape[2:]))\n",
    "                weights = F.interpolate(weights,\n",
    "                                        size=self.images_to_tensors[0].shape[2:])\n",
    "#                 current_fusion = torch.zeros(self.images_to_tensors[0].shape)\n",
    "#                 for idx, tensor_img in enumerate(self.images_to_tensors):\n",
    "#                     current_fusion += tensor_img * weights[:,idx]\n",
    "#                 if max_fusion is None:\n",
    "#                     max_fusion = current_fusion\n",
    "#                 else:\n",
    "#                     max_fusion = torch.max(max_fusion, current_fusion)\n",
    "\n",
    "            output = np.squeeze(weights.cpu().numpy())\n",
    "            if output.ndim == 3:\n",
    "                output = np.transpose(output, (1, 2, 0))\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        \n",
    "    def _RGB_to_YCbCr(self, img_RGB):\n",
    "            \"\"\"\n",
    "            A private method which converts an RGB image to YCrCb format\n",
    "            \"\"\"\n",
    "            img_RGB = img_RGB.astype(np.float32) / 255.\n",
    "            return cv2.cvtColor(img_RGB, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "    def _YCbCr_to_RGB(self, img_YCbCr):\n",
    "            \"\"\"\n",
    "            A private method which converts a YCrCb image to RGB format\n",
    "            \"\"\"\n",
    "            img_YCbCr = img_YCbCr.astype(np.float32)\n",
    "            return cv2.cvtColor(img_YCbCr, cv2.COLOR_YCrCb2RGB)\n",
    "\n",
    "    def _is_gray(self, img):\n",
    "            \"\"\"\n",
    "            A private method which returns True if image is gray, otherwise False\n",
    "            \"\"\"\n",
    "            print(\"in _is_gray\")\n",
    "            if len(img.shape) < 3:\n",
    "                return True\n",
    "            if img.shape[2] == 1:\n",
    "                return True\n",
    "            b, g, r = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "            if (b == g).all() and (b == r).all():\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def _softmax(self, tensor):\n",
    "            \"\"\"\n",
    "            A private method which compute softmax ouput of a given tensor\n",
    "            \"\"\"\n",
    "            tensor = torch.exp(tensor)\n",
    "            tensor = tensor / tensor.sum(dim=1, keepdim=True)\n",
    "            return tensor\n",
    "\n",
    "    def _tranfer_to_tensor(self):\n",
    "            \"\"\"\n",
    "            A private method to transfer all input images to PyTorch tensors\n",
    "            \"\"\"\n",
    "            self.images_to_tensors = []\n",
    "            for image in self.normalized_images:\n",
    "                np_input = image.astype(np.float32)\n",
    "                if np_input.ndim == 2:\n",
    "                    np_input = np.repeat(np_input[None, None], 3, axis=1)\n",
    "                else:\n",
    "                    np_input = np.transpose(np_input, (2, 0, 1))[None]\n",
    "                if self.device == \"cuda\":\n",
    "                    self.images_to_tensors.append(torch.from_numpy(np_input).cuda())\n",
    "                else:\n",
    "                    self.images_to_tensors.append(torch.from_numpy(np_input)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images: (512, 512, 3)\n",
      "Input images: (512, 512, 3)\n",
      "in _is_gray\n",
      "in _is_gray\n",
      "in _is_gray\n",
      "in _is_gray\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the methods for Siamese\n",
    "input_images = []\n",
    "input_images.append(cv2.imread('jpg/mri_registered.jpg'))\n",
    "input_images.append(cv2.imread('jpg/ct.jpg'))\n",
    "print(\"Input images:\", input_images[0].shape)\n",
    "print(\"Input images:\", input_images[1].shape)\n",
    "# Compute fusion image\n",
    "FU = Fusion(input_images)\n",
    "fusion_img = FU.fuse()\n",
    "# Write fusion image\n",
    "cv2.imwrite('jpg/weight_map.jpg', fusion_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
